{
  "translation": " Hello everyone, welcome to AWS tutorials. In AWS tutorials we provide workshops and exercises to learn about AWS services. These workshops and exercises are published to our website aws hyphen dozo.com. Today we are going to talk about how you can talk to Amazon RDS database service from AWS Lambda and how you should be architecting about it and what kind of options you can choose for the programming. Now, so let's get started. So before we go further in the tutorials, when you're talking to RDS from Lambda then actually there are various, there are many choices and various variables. So for instance, you have different data types, you have different programming language. So just to set the expectation right, here we will be talking about MySQL and PostgreSQL as a database engines and that will also cover your Aurora, whether it's Aurora provisioned or Aurora serverless and we will use Python as a programming language. So if it still sounds interesting to you, you can continue in the tutorial, but if you think, Hey, I was looking for a different database engine or programming language, then I mean you can still watch it, but probably not suitable for what is specific you are looking for. Okay. So having discussed the scope, let's move on. So what are the use cases when Lambda will like to talk to RDS? Okay. I mean, and use cases are pretty simple and straightforward. So one typical use case you will find is that you are building API or it could be a webpage as well, which is then talking to Lambda function and then Lambda function is then talking to Amazon RDS. Yeah. That could be one use case. Other use case could be a batch processing. So for instance, you have some data into RDS and then you want AWS Lambda to process it and you can use cloud watch for any kind of scheduled run of the Lambda function. Now this can also be an event based processing as well. But here keep in mind that when you're going for doing data processing using Lambda, then in that case, of course you're not looking for a very large amount of a large duration of processing because then Lambda has resource limitations in terms of how much memory it can go up to and how much execution time it can go up to. So if you have some very transactional event based or schedule based data processing, then you can go for batch processing in Lambda, data processing in Lambda and in which case Lambda will talk to RDS. So these are two typical use cases you will find when you have to make Lambda and RDS talk to each other. So keeping these use cases in mind, how does the whole development take place? And the first decision you have to take, especially again talking in the context of MySQL and PostgreSQL as a database engines and Python as a programming language, you have to choose a library for the coding. So AWS Lambda can talk to RDS, but for that it needs a Python module, Python library, which it can use to talk to RDS. Yeah. Because Lambda out of box does not have native API to talk to, native API to talk to PostgreSQL or MySQL. So you have to create a custom package with these Python modules or library, which then Lambda can use to talk to RDS. Now, when it comes to which library to use, which module to use, there are various choices available and choices sometimes depend on which database engine you are talking about. So for instance, if you are talking to MySQL, then you might want to use PyMySQL module or library. If you are talking to PostgreSQL, then you can use a PsyCops2 module. And there are modules which are actually, you can say, which are kind of a universal or which are like, what do you call that? Yeah. Universal to both MySQL and PostgreSQL. So it doesn't matter. It's a MySQL or PostgreSQL. Those modules work for both. And AWS Wrangler is one such module or library, which can talk to MySQL and PostgreSQL both. Okay. So roughly you choose a library like this when you're talking to RDS and you choose your Python library depending on what database engine you are working with. Okay. Now I'm not saying that these are the only libraries available. I mean, there are of course things beyond these library, but these are, I will say, the most popular libraries people use when doing Python based programming to talk to RDS engine. Or if I can make it more generic, when people want to talk to MySQL or PostgreSQL instances, these are the typical libraries people use. So first choice is about the library. And then actually architecture depends on what library you choose. Yeah. I mean, you might say, wow, really? And an answer is yes. Whether you're choosing MySQL, PyMySQL or Psycops2 as the library or you use AWS Wrangler as a library, that is going to change your design, your architecture. And let me explain you that part. So let's take example that suppose you're going to use PyMySQL or Psycops2 as the library. So let's take examples. Suppose you have got AWS cloud where in a VPC you are running an RDS, and then you have your Lambda function, which is using one of these two libraries and it wants to talk to the RDS. So first thing you do in such a design that you create secret inside AWS Secrets Manager. You don't want to hard code your RDS login name and password in Lambda function. So you might want to use AWS Secrets Manager to store the login name and password for the RDS engine. Then what happens? Your Lambda function has to be a VPC enabled Lambda function. So your Lambda function will be associated with the VPC where your RDS database instance is running. And by doing so, it will have an ENI in the same VPC as your RDS. And that way your Lambda function can talk to RDS using this ENI. But making this Lambda function VPC enabled or VPC associated actually creates a complexity because this Lambda function is now using this ENI to talk to the resources. So this ENI will also talk to the Secrets Manager, but there is no path because if both were without VPC, suppose this Lambda function was without VPC, it can talk to AWS Secrets Manager directly, not a problem. But now this Lambda function is ENI based. That means it is a VPC enabled and all the communication will happen through this ENI. That means this ENI should be able to talk to your Secrets Manager. And that is possible only when you create, when there are two choices available, either you create a VPC endpoint or you associate the subnet of the VPC with NAT gateway. But I'm not going to talk about NAT gateway in this case because in case of NAT gateway, your traffic goes through internet. And I don't prefer that as a choice because then it has performance implication plus security wise also people might not like routing their traffic through the internet. So the best choice is that you can create a VPC endpoint. So in this VPC, you create a VPC endpoints for the Secrets Manager. And now this ENI can use this VPC endpoints to make a private communications to the Secrets Manager. So that's how at high level you architect. I mean, there are other like roles and security groups involved, but I did not want to add it here because the diagram would have become more complex. But the key message I want to draw here is that in order to talk to RDS instance, Lambda function needs to have an ENI in the VPC. And since Lambda function is talking through the ENI, this ENI needs to find a way to connect to the Secrets Manager. And for that, you need to have a VPC endpoint for the Secrets Manager. So with this kind of architecture, your Lambda function can talk to Secrets Manager to fetch the credentials for the RDS and then can talk to the RDS engine using one of these two packages to work with the data. Now, this is how the architecture look like if you use PyMySQL or Cycops 2 packages. Now what if you are using AWS Wrangler? Because AWS Wrangler is also a package choice. And personally, if you ask me, I prefer AWS Wrangler because this is one library which you will see it simplifies your code a bit. And it also it simplifies the code a bit and it also actually yeah, I mean, it can work with any kind of database engine. So I really don't need to think my coding is specific to a database engine because your coding, your syntaxes change depending on you're talking to the MySQL or PostgreSQL. In case of Data Wrangler, I have not to worry about that part. So I like, I prefer Data Wrangler because it gives me a single way of coding and it can work with both database engines. Now in this case, AWS Wrangler does not use Secrets Manager. That's the first change. So it uses AWS Glue Connection. So you can use AWS Glue Connections to connect to the RDS instance. So you provide your endpoint and credentials information in AWS Glue Connection. And now your Lambda function will have this ENI in the VPC to talk to the RDS engine. And since this ENI has to talk to the Glue in order to fetch the credential and endpoint information, we have to provide an endpoint here for the Glue and also for the S3. Now Glue, you say, okay, makes sense because this ENI has to talk to the Glue Connection. So I need to provide a Glue endpoint. But why I need to provide an S3 endpoint? The reason is that Glue Connection will also have an ENI like this to test the connection and Glue needs S3 gateway for internal use. And for that purpose, to make Glue Connection work, you need to provide an S3 endpoint, so that Glue can also work here. So in case of Glue, if you're using AWS Wrangler, then you keep your endpoint and credential information in the Glue Connection. Then you, of course, attach your Lambda function to a VPC where the RDS instance is running. And then you create a VPC endpoint for both Glue and S3. And then this ENI will use this Glue VPC endpoint to talk to this Glue to fetch all the connection and endpoint related information. So as I told you earlier, depending on which package you choose, your architecture changes a bit. So in principle, the whole design works in the same way, but services like Glue will come in and in case of AWS Wrangler, while if you're using other packages, then in that case, you have to use a Secrets Manager to store your credentials and probably endpoint information as well. So having understood the architectural differences depending on which packages you're choosing, let's move on. So how do I package PyMySQL with the, or a Psycops G2 with Lambda? And I'm going to use PyMySQL as an example over here. So what you do for that is that first you have to, so basically what you're doing is you're writing your code into Lambda, but since Lambda needs PyMySQL library, PyMySQL library has to be packaged with the Lambda code and that has to be uploaded to the Lambda function definition. So first thing you do, you create a, for instance, a local deployment package for your, for PyMySQL or Psycops G2. Now in this case, if to deploy a local package, you use a command like pip install, t is a target and dot means in the current folder and this is my package name and it will actually basically create a PyMySQL distribution package on your local folder or all the location you provide right now it's dot that means give me in my current directory. So first you create a local package like that. After that you have two choices and one choice is that you jip everything together and upload to the Lambda function. So for instance, this is my PyMySQL folder, this is my distribution folder and this is my Lambda function code and I simply select this, these two folders and my Lambda function code put into a zip file and simply upload to my Lambda function. Yeah, you can very well do that and in that case, yeah, you can use these libraries along with the Lambda function. By the way, we are going to, I'm going to demonstrate this whole thing as well so you will see how exactly it works. The method number two, which I will say is the preferred method is of course you have to create a local deployment but then what you can do is that these distribution folders of the PyMySQL you can zip and upload that as a Lambda layer. So instead of zipping your Lambda function code plus library both together into one, you first take your PyMySQL folders and upload that to a Lambda layer and then that Lambda layer you can add to the Lambda function and that way Lambda function code can still use PyMySQL library. Now method two I will say is preferred method because in this case, this layer you create for PyMySQL can be used by the other Lambda functions as well. If you put everything together then your package is very much local to that Lambda function only. It cannot be reused outside but if you configure that as a layer then your package is something which can be used by other Lambda functions as well. Okay, so that's how you do the packaging of the libraries with the Lambda function and I'm going to show you this as well so yeah, you will be okay. Now moving on, now so what, how does the AWS Wrangler packaging work? So in case of AWS Wrangler packaging, the good thing is that you don't need to create a local package. I mean you can create with the source code but you don't need to actually because if you go to AWS Wrangler website you will find that there are jips already created for different versions of the Python. So like you can see here 3.6 to 3.8. So all you need to do is that simply download a jiff file depending on which version of Python you are going to use. Then you upload this jiff to a Lambda layer. Simply configure a Lambda layer using this jiff file so that becomes your AWS Wrangler layer and then you can simply add that layer to the Lambda function. Now if you are not familiar with how layer works in Lambda, actually I have a separate tutorial about using layers within Lambda. So you might want to search in my YouTube channel about AWS Lambda layers and that there you'll probably learn more about how to use Lambda layers. Okay, but basically using AWS Wrangler is even simpler because in this case you don't need to do any local deployment of the package. You can simply download the jiff file, configure a Lambda layer and simply add that Lambda layer to the Lambda function and do the coding. That's how you do the packaging of the AWS Wrangler. So moving on. So how does my connection code work? So now we understood the architecture at high level, we understood the packages to be used, we understood how to do the packaging of the libraries with the Lambda function. Now how does the coding look like? So if you are using PyMySQL or PsyCorp G2, I feel the number of lines of code written are more than what you'll do with the AWS Wrangler. I'm going to show you here. So for instance, let's take this example, okay. And I'm using in this case PyMySQL as an example. So first we import PyMySQL library and few other libraries as well. Then you can, in this case, in this example, in fact, code snippet, we have set up the RDS endpoint, RDS database name, your secret name, your region, everything in the involvement variable. So we pick this information from the involvement variable because they are like non sensitive information. Then we are creating a Boto session. And once you create the Boto session, actually you then talk to, yeah, you call, you make a call to the, you make a client to the secrets manager and then you say, I want to get secret value for this particular secret. And then from the secret value, you can fetch the username and password. So these are two key values I have configured in the secrets manager in this particular example. And then you simply make a connection where you provide the host name, username, database, password, your, and your cursor class. And that's it. You make the connection. So you have to write some, in my opinion, some significant amount of code. Well, not complex when using, and compared to again, AWS Wrangler when using PyMySQL or CyCops, CyCops G2. Now if I'm using AWS Wrangler, then my code looks like this. So in this case, I have got my, I need to configure a glue connection as I showed you in the architecture page. So all you're doing is that probably you might want to put this glue connection name in the environment variables. So fetch the glue connection name. Then you use AWS Wrangler library and simply say, I want to connect to connect using the glue connection. And with this library, this few lines of code, actually you can connect to your database using glue connection. And that's why I'm saying that compared to other two libraries, I, I happen to write fewer lines of code when I'm using AWS Wrangler. Okay. So that's a, that's a difference in the coding depending on which library you choose for your programming. Now, what are you building today? So it's time to show you a demo. And in this case, what we're doing here is that we'll have our RDS serverless instance. In fact, Aurora serverless running for, which is micro SQL, my SQL compatible. And then we have a Lambda function. And then I'll show you how you can use by my SQL and AWS Wrangler, both way to connect to the, connect to the RDS engine. And, and I will also run a simple SQL statement. Okay. So let's it's time for the demo and let's move on to the demo. So moving on to the demo, let me show you what is already configured in order to save some time. So you can see here that I have already configured an Aurora serverless which is my SQL compatible. And this one, this end point is actually running in my default VPC using all three subnets. And this is my security group. So if I show you the security group configuration it should be somewhere here. Yeah. Here's my security group configuration. So if you look at over here it is actually taking my SQL traffic only from the default VPC. And then it is also doing self refreshing, self referencing, which is one of the requirement when you're trying to use a glue connection. Now here is in this RDS instance, I have created a database, dojo DB, and there I have created a table, my friends. And if I see a number records in my friends, you will find that, well, okay. So this one is okay. There's a connection error. So probably I need to connect again. Let me connect one more time. Okay. It looks like my connection is broken. So let me connect one more time. And okay. And change database. Okay. Just to reestablish a new connection. And let's simply say select star from my friends. And you run it. And hopefully, yeah, you can see it is empty. So this is fine. So this is my table I'm going to work with. Now I have created three endpoints since I'm going to use PyMySQL and also AWS triangular. So I have created three endpoints. So one end point is for glue. One end point is for secrets manager and one is for S3. And if you click on one of the endpoints, you will find that it is using all three subnets where same VPC where the RDS engine is, it's using the same security group. Yeah. So yeah, that's how the configuration look like. And same is for secrets manager and S3 as well. Now you have seen the security group. Now I created a couple of IAM roles. So one I created a dozer glue role. And this dozer glue role will be used by actually data connection. Glue connection, in fact. So that has got only one permission that is AWS glue service role. And then I have created another role for the Lambda and Lambda has got actually three permissions. One is glue service role so that it can talk to the glue connection. It has got a Lambda basic execution role so that it can create a cloud watch logs. And third one it has got Lambda VPC access is beautiful role so that it can associate itself to the VPC and can create ENIs and those kinds of things. Now I also configured a dojo, a glue connection. And you can see that this is a JDBC kind of connection. This is the end point. I'm talking to same VPC where my RDS instances, I choose one of the subnet and same security group. And I prodded login name and password. And this connection was stabilized and tested to say that it is working. So these are things I have already configured so that we can focus on the Lambda coding part. So let's start with the Lambda coding. And first thing I will do is that I will actually use AWS Wrangler. So this is my Lambda console. If I go to my layers, I have configured already a layer for AWS Wrangler. And again, if you want to know how it works, actually I have another tutorials which talks about using AWS Wranglers with Lambda. You might want to refer to that and that will tell you how to use AWS, how to configure AWS Wrangler with Lambda using Lambda layer. Okay. But here in this case, I have already got a version 3.8 compatible AWS Wrangler layer configured for the Lambda. So now I can start creating my function. So I create my first function and this function I will author from the scratch and I'll say this is my, you can say, okay, Wrangler function. And then this one is Python 3.8. And then I changed my execution role to the IAM role I created for the Lambda. In this case, it is Dojo Lambda role and I create the function. And the function should be created in no time. The function has now been created. Now, first thing I need to do is that I need to, since in this method, I'm going to use AWS Wrangler. So first thing I need to do, I need to add the layer, which I have configured for the Wrangler, AWS Wrangler. So I click on the layers link and I say, I want to add a layer and I say, let's use a custom layer. This is my AWS Wrangler layer. I've got only one version. Let's add it. So now my AWS Lambda function has got a layer configured over here. Now the next step is to once my layer is configured, since I'm talking to database, I think I would like to go and increase the timeout period to a little, little larger. So first I go to my journal configuration over here and say, okay, let's increase that to, okay, just to be safe for slide one minute, but it doesn't need one minute, probably five, 10 seconds is enough that two for the first execution. But I increase the timeout and then I want to attach this Lambda function to the VPC because it has to talk to the RDS. So I need to attach this Lambda function to the VPC. So I go to the VPC section over here. I want to say edit and I want to use a default VPC because that's where my RDS database is. And I can select all three subnets if you want. And I can select here the MySQL security group, the same security group. Other things are using, I simply unified everything to same security group. So I simply attach that security group and I save it. Now what's going to do that is going to associate my Lambda function with the VPC. So what happens behind the scene is that actually one ENI will be created for this Lambda function in the same VPC where, in the same VPC where your RDS database is running. And through that ENI, now my Lambda function can now make call to the RDS actually. So let's wait till this Lambda function gets associated with the VPC of the RDS database instance. And then we will simply upload, change the code and then we will test how it works. So yeah, let's wait till this configuration. So I have paused my video in the meantime, but now my Lambda function is associated with the VPC. Now I can change, I'm all good. I can change my code. You can see that it has got a VPC configuration done. So I can go to the code section here and say, and simply say, I want to change the code and I replace this code. Let me go a little up. I replace this code with another code. So here we go. So I will provide the snippet of this code in the description box below, but it's pretty straightforward code guys. So I'm simply importing the AWS Angular function. Then I'm using Dojo connections. In this case, I have not put Dojo connection name or like my glue connection name in the involvement variable just to save some time. I'm simply hard coding it over here, but I'm using data regular library to connect to the MySQL instance using the glue connection. Then I'm creating a cursor and is getting this small query, which is going to insert a recording to my table, commit and close. That's it. Pretty straightforward. So let's deploy this code and then we are going to test it. So deployment is done. Now we are going to test it. So when you click on test, it's asking me to configure a test. So there is no input. So I can say, okay, this is simple test and yeah, let's create this test. And then you simply run the test and run the test. Hopefully everything is fine and you can see that it has responded back there with data inserted. So if I go to my RDS instance and run this query again, I should see a code. I should see a row inserted over here. First name one and last name one. Okay. So again, I will provide this code snippet. So you can play with yourself in the description box below. Now having tested one Lambda function, let's go and create the other Lambda function, which is going to use actually, pie my SQL library. So for that, first we will create a package for pie my SQL. So here is my folder where I have got my Lambda function already actually created. And I'll, I'll, when I upload you, I'll show you the code, but I have to configure my package and zip it. And then I can, then I can create my whole Lambda function. So I'm not using the layer method for the second example, because I want to save some time to be very honest, but let's run, let's create a local deployment package. And for that, we will simply run a command and the command is pip install, hyphen T dot pie my SQL. So basically this command. So it's saying that, Hey, I want to install pie my SQL, but my target is a local directory. Simple. Yeah. And I am in C AWS dojo folder, and I'm simply running this command and it will actually create a local deployment distribution for pie my SQL. So you can see here, these folders got created. Now I need to simply select these two folders and my Lambda function code and simply send it to zip. Okay. So I simply send it to zip and I'm ready. Now let's go back to my Lambda function and say, I want to create a new Lambda function. And this one, I will say SQL Lambda. Yeah. SQL Lambda. Yeah. And if I come down here, I can select AWS Python 3.8 permission wise. Again, nothing changes. I gave the same permission in this case and I give it dojo. Oops. Where is that? I do that. Dojo Lambda function, Lambda role. And I create the role. So once I have created the role, my role is, is, is created. Now let's do well, you can see here that yeah, right now this one is using the default one, but this is not what I want to use. So what I will do is that I will simply go over here and say, I want to upload a package from my local directory. And in this case, I come over here and okay. I'm it's, I think it's going to a different folder. So I will simply ask it to go to another folder. And then this was my zip file. I simply save it. And what it is going to do, it's going to take my local local package and it's going to upload it to the Lambda function. Again, guys, I'm not using the best practice here. I ideally I should have put zip these two folders put into a layer and then add that layer over here. But that would have taken some time. So I tried to avoid that for the demo purposes. So let's see the code. If I wrap the line over here, you can see the lines of course are different here. So in this case, I'm using PI my SQL, and then I'm using PI my SQL.connect method to actually connect. And I have to provide my host name, username, database name, password, and my cursor class. And again, another best practice I'm not using here is that I should have put my database name and host name into environment variable because they are non sensitive information and my username and password into secrets manager. But again, I try to save some time for the video. So I simply hardcoded those values over here, but you should not be doing that. Okay. I'm sorry. I'm showing this thing here, but yeah, you don't do that. You keep your information in the secrets manager. And then yeah, I'm simply, I'm creating a cursor is getting a SQL statement with the cursor. Unless for the sake of testing, make it a two and two, and then yeah, commit and close. And that's it. So I simply going to deploy this thing here. So now my code is deployed, but I'm not ready yet because I need to configure my VPC endpoint. VPC is not VPC endpoint. I need to associate VPC with the Lambda function. And I also need to increase time out a bit. So let's go to the configuration and let's first go to the general configuration. I want to increase the timeout. Let's again choose a simple path, make it one minute, save it. Then you come down to the VPC section over here. Say I want to edit it and select my default VPC, my all three subnets. Yeah. It has to be your best practice. And then you select, yeah, my SQL security group over here. So that configuration is same whether you're whichever package you're using and you simply save it. Now, again, it is going to associate my VPC with my Lambda function with the VPC and it's going to take some, going to take some while. So we'll wait till the Lambda function gets associated with the, okay. So my Lambda function is not associated with the VPC and it happened really fast this time. Okay. So now my configuration is ready. I have got my Lambda function associated with the VPC and my code is also uploaded, which is actually my PyMySQL distribution has been packaged along with my code. And my code is pretty straightforward, which is connecting to the instance. And then I'm simply executing a simple insert statement. So let's run this code and hopefully it works. Again, it will ask to configure the test event. And I simply create it and then I test it and hopefully it works. Yeah, it worked. And if I go back to my RDS database and I run my query again, you can see that it would have got another row inserted. And that is first name to, and last name to, which showed that my Lambda function has worked. So this was an example guys, how you can configure your Lambda function to work with RDS and you learned two methods. One is using packages like PyMySQL to talk to the talk to the RDS engine. And I also learned how to use AWS data Wrangler to talk to the RDS databases. And I told you that, I mean, I don't, I mean, I'm not recommending use one over another, but if you ask my personal choice, I prefer AWS Wrangler over the other packages because it's a, yeah, it's pretty simple to use and number of lines of code I write is less. And I can also use Panda data frame to manipulate my data, which actually makes life easy, especially if you're trying to perform any kind of data processing kind of operation. So that was all for today, guys. Hope you liked this video. And if you like, please click on the like button, please subscribe to my channel. And if you visit our website, aws.dojo.com, you'll find many other videos and tutorials similar to like this. We always look forward to your feedback, whether it is for a new content or you want to discuss something we already talked about in a tutorial or we have made some mistake and you want to correct us, please do so. So we always look forward to your feedback to improve our quality to improve our delivery to you. So please provide your feedback in the description box of the YouTube channel. And I promise to come back again in a week's time with another such tutorial. In the meantime, have a nice day and stay safe. Bye bye.",
  "transcription": " Hello everyone, welcome to AWS tutorials. In AWS tutorials we provide workshops and exercises to learn about AWS services. These workshops and exercises are published to our website aws hyphen dozo.com. Today we are going to talk about how you can talk to Amazon RDS database service from AWS Lambda and how you should be architecting about it and what kind of options you can choose for the programming. Now, so let's get started. So before we go further in the tutorials, when you're talking to RDS from Lambda then actually there are various, there are many choices and various variables. So for instance, you have different data types, you have different programming language. So just to set the expectation right, here we will be talking about MySQL and PostgreSQL as a database engines and that will also cover your Aurora, whether it's Aurora provisioned or Aurora serverless and we will use Python as a programming language. So if it still sounds interesting to you, you can continue in the tutorial, but if you think, Hey, I was looking for a different database engine or programming language, then I mean you can still watch it, but probably not suitable for what is specific you are looking for. Okay. So having discussed the scope, let's move on. So what are the use cases when Lambda will like to talk to RDS? Okay. I mean, and use cases are pretty simple and straightforward. So one typical use case you will find is that you are building API or it could be a webpage as well, which is then talking to Lambda function and then Lambda function is then talking to Amazon RDS. Yeah. That could be one use case. Other use case could be a batch processing. So for instance, you have some data into RDS and then you want AWS Lambda to process it and you can use cloud watch for any kind of scheduled run of the Lambda function. Now this can also be an event based processing as well. But here keep in mind that when you're going for doing data processing using Lambda, then in that case, of course you're not looking for a very large amount of a large duration of processing because then Lambda has resource limitations in terms of how much memory it can go up to and how much execution time it can go up to. So if you have some very transactional event based or schedule based data processing, then you can go for batch processing in Lambda, data processing in Lambda and in which case Lambda will talk to RDS. So these are two typical use cases you will find when you have to make Lambda and RDS talk to each other. So keeping these use cases in mind, how does the whole development take place? And the first decision you have to take, especially again talking in the context of MySQL and PostgreSQL as a database engines and Python as a programming language, you have to choose a library for the coding. So AWS Lambda can talk to RDS, but for that it needs a Python module, Python library, which it can use to talk to RDS. Yeah. Because Lambda out of box does not have native API to talk to, native API to talk to PostgreSQL or MySQL. So you have to create a custom package with these Python modules or library, which then Lambda can use to talk to RDS. Now, when it comes to which library to use, which module to use, there are various choices available and choices sometimes depend on which database engine you are talking about. So for instance, if you are talking to MySQL, then you might want to use PyMySQL module or library. If you are talking to PostgreSQL, then you can use a PsyCops2 module. And there are modules which are actually, you can say, which are kind of a universal or which are like, what do you call that? Yeah. Universal to both MySQL and PostgreSQL. So it doesn't matter. It's a MySQL or PostgreSQL. Those modules work for both. And AWS Wrangler is one such module or library, which can talk to MySQL and PostgreSQL both. Okay. So roughly you choose a library like this when you're talking to RDS and you choose your Python library depending on what database engine you are working with. Okay. Now I'm not saying that these are the only libraries available. I mean, there are of course things beyond these library, but these are, I will say, the most popular libraries people use when doing Python based programming to talk to RDS engine. Or if I can make it more generic, when people want to talk to MySQL or PostgreSQL instances, these are the typical libraries people use. So first choice is about the library. And then actually architecture depends on what library you choose. Yeah. I mean, you might say, wow, really? And an answer is yes. Whether you're choosing MySQL, PyMySQL or Psycops2 as the library or you use AWS Wrangler as a library, that is going to change your design, your architecture. And let me explain you that part. So let's take example that suppose you're going to use PyMySQL or Psycops2 as the library. So let's take examples. Suppose you have got AWS cloud where in a VPC you are running an RDS, and then you have your Lambda function, which is using one of these two libraries and it wants to talk to the RDS. So first thing you do in such a design that you create secret inside AWS Secrets Manager. You don't want to hard code your RDS login name and password in Lambda function. So you might want to use AWS Secrets Manager to store the login name and password for the RDS engine. Then what happens? Your Lambda function has to be a VPC enabled Lambda function. So your Lambda function will be associated with the VPC where your RDS database instance is running. And by doing so, it will have an ENI in the same VPC as your RDS. And that way your Lambda function can talk to RDS using this ENI. But making this Lambda function VPC enabled or VPC associated actually creates a complexity because this Lambda function is now using this ENI to talk to the resources. So this ENI will also talk to the Secrets Manager, but there is no path because if both were without VPC, suppose this Lambda function was without VPC, it can talk to AWS Secrets Manager directly, not a problem. But now this Lambda function is ENI based. That means it is a VPC enabled and all the communication will happen through this ENI. That means this ENI should be able to talk to your Secrets Manager. And that is possible only when you create, when there are two choices available, either you create a VPC endpoint or you associate the subnet of the VPC with NAT gateway. But I'm not going to talk about NAT gateway in this case because in case of NAT gateway, your traffic goes through internet. And I don't prefer that as a choice because then it has performance implication plus security wise also people might not like routing their traffic through the internet. So the best choice is that you can create a VPC endpoint. So in this VPC, you create a VPC endpoints for the Secrets Manager. And now this ENI can use this VPC endpoints to make a private communications to the Secrets Manager. So that's how at high level you architect. I mean, there are other like roles and security groups involved, but I did not want to add it here because the diagram would have become more complex. But the key message I want to draw here is that in order to talk to RDS instance, Lambda function needs to have an ENI in the VPC. And since Lambda function is talking through the ENI, this ENI needs to find a way to connect to the Secrets Manager. And for that, you need to have a VPC endpoint for the Secrets Manager. So with this kind of architecture, your Lambda function can talk to Secrets Manager to fetch the credentials for the RDS and then can talk to the RDS engine using one of these two packages to work with the data. Now, this is how the architecture look like if you use PyMySQL or Cycops 2 packages. Now what if you are using AWS Wrangler? Because AWS Wrangler is also a package choice. And personally, if you ask me, I prefer AWS Wrangler because this is one library which you will see it simplifies your code a bit. And it also it simplifies the code a bit and it also actually yeah, I mean, it can work with any kind of database engine. So I really don't need to think my coding is specific to a database engine because your coding, your syntaxes change depending on you're talking to the MySQL or PostgreSQL. In case of Data Wrangler, I have not to worry about that part. So I like, I prefer Data Wrangler because it gives me a single way of coding and it can work with both database engines. Now in this case, AWS Wrangler does not use Secrets Manager. That's the first change. So it uses AWS Glue Connection. So you can use AWS Glue Connections to connect to the RDS instance. So you provide your endpoint and credentials information in AWS Glue Connection. And now your Lambda function will have this ENI in the VPC to talk to the RDS engine. And since this ENI has to talk to the Glue in order to fetch the credential and endpoint information, we have to provide an endpoint here for the Glue and also for the S3. Now Glue, you say, okay, makes sense because this ENI has to talk to the Glue Connection. So I need to provide a Glue endpoint. But why I need to provide an S3 endpoint? The reason is that Glue Connection will also have an ENI like this to test the connection and Glue needs S3 gateway for internal use. And for that purpose, to make Glue Connection work, you need to provide an S3 endpoint, so that Glue can also work here. So in case of Glue, if you're using AWS Wrangler, then you keep your endpoint and credential information in the Glue Connection. Then you, of course, attach your Lambda function to a VPC where the RDS instance is running. And then you create a VPC endpoint for both Glue and S3. And then this ENI will use this Glue VPC endpoint to talk to this Glue to fetch all the connection and endpoint related information. So as I told you earlier, depending on which package you choose, your architecture changes a bit. So in principle, the whole design works in the same way, but services like Glue will come in and in case of AWS Wrangler, while if you're using other packages, then in that case, you have to use Secrets Manager to store your credentials and probably endpoint information as well. So having understood the architectural differences depending on which packages you're choosing, let's move on. So how do I package PyMySQL with the, or a Psycops G2 with Lambda? And I'm going to use PyMySQL as an example over here. So what you do for that is that first you have to, so basically what you're doing is you're writing your code into Lambda, but since Lambda needs PyMySQL library, PyMySQL library has to be packaged with the Lambda code and that has to be uploaded to the Lambda function definition. So first thing you do, you create a, for instance, a local deployment package for your, for PyMySQL or Psycops G2. Now in this case, if to deploy a local package, you use a command like pip install, t is a target and dot means in the current folder and this is my package name and it will actually basically create a PyMySQL distribution package on your local folder or all the location you provide right now it's dot that means give me in my current directory. So first you create a local package like that. After that you have two choices and one choice is that you jip everything together and upload to the Lambda function. So for instance, this is my PyMySQL folder, this is my distribution folder and this is my Lambda function code and I simply select this, these two folders and my Lambda function code put into a zip file and simply upload to my Lambda function. Yeah, you can very well do that and in that case, yeah, you can use these libraries along with the Lambda function. By the way, we are going to, I'm going to demonstrate this whole thing as well so you will see how exactly it works. The method number two, which I will say is the preferred method is of course you have to create a local deployment but then what you can do is that these distribution folders of the PyMySQL you can zip and upload that as a Lambda layer. So instead of zipping your Lambda function code plus library both together into one, you first take your PyMySQL folders and upload that to a Lambda layer and then that Lambda layer you can add to the Lambda function and that way Lambda function code can still use PyMySQL library. Now method two I will say is preferred method because in this case, this layer you create for PyMySQL can be used by the other Lambda functions as well. If you put everything together then your package is very much local to that Lambda function only. It cannot be reused outside but if you configure that as a layer then your package is something which can be used by other Lambda functions as well. Okay, so that's how you do the packaging of the libraries with the Lambda function and I'm going to show you this as well so yeah, you will be okay. Now moving on, now so what, how does the AWS Wrangler packaging work? So in case of AWS Wrangler packaging, the good thing is that you don't need to create a local package. I mean you can create with the source code but you don't need to actually because if you go to AWS Wrangler website you will find that there are jips already created for different versions of the Python. So like you can see here 3.6 to 3.8. So all you need to do is that simply download a jiff file depending on which version of Python you are going to use. Then you upload this jiff to a Lambda layer. Simply configure a Lambda layer using this jiff file so that becomes your AWS Wrangler layer and then you can simply add that layer to the Lambda function. Now if you are not familiar with how layer works in Lambda, actually I have a separate tutorial about using layers within Lambda. So you might want to search in my YouTube channel about AWS Lambda layers and that there you'll probably learn more about how to use Lambda layers. Okay, but basically using AWS Wrangler is even simpler because in this case you don't need to do any local deployment of the package. You can simply download the jiff file, configure a Lambda layer and simply add that Lambda layer to the Lambda function and do the coding. That's how you do the packaging of the AWS Wrangler. So moving on. So how does my connection code work? So now we understood the architecture at high level, we understood the packages to be used, we understood how to do the packaging of the libraries with the Lambda function. Now how does the coding look like? So if you are using PyMySQL or PsyCorp G2, I feel the number of lines of code written are more than what you'll do with the AWS Wrangler. I'm going to show you here. So for instance, let's take this example, okay? And I'm using in this case PyMySQL as an example. So first we import PyMySQL library and few other libraries as well. Then you can, in this case, in this example, in fact, code snippet, we have set up the RDS endpoint, RDS database name, your secret name, your region, everything in the involvement variable. So we pick this information from the involvement variable because they are like non sensitive information. Then we are creating a Boto session. And once you create the Boto session, actually you then talk to, yeah, you call, you make a call to the, you make a client to the secrets manager and then you say, I want to get secret value for this particular secret. And then from the secret value, you can fetch the username and password. So these are two key values I have configured in the secrets manager in this particular example. And then you simply make a connection where you provide the host name, username, database, password, your, and your cursor class. And that's it. You make the connection. So you have to write some, in my opinion, some significant amount of code. Well, not complex when using, and compared to again, AWS Wrangler when using PyMySQL or CyCops, CyCops G2. Now if I'm using AWS Wrangler, then my code looks like this. So in this case, I have got my, I need to configure a glue connection as I showed you in the architecture page. So all you're doing is that probably you might want to put this glue connection name in the environment variables. So fetch the glue connection name. Then you use AWS Wrangler library and simply say, I want to connect to connect using the glue connection. And with this library, this few lines of code, actually you can connect to your database using glue connection. And that's why I'm saying that compared to other two libraries, I, I happen to write fewer lines of code when I'm using AWS Wrangler. Okay. So that's a, that's a difference in the coding depending on which library you choose for your programming. Now, what are you building today? So it's time to show you a demo. And in this case, what we're doing here is that we'll have our RDS serverless instance. In fact, Aurora serverless running for, which is micro SQL, my SQL compatible. And then we have a Lambda function. And then I'll show you how you can use by my SQL and AWS Wrangler, both way to connect to the, connect to the RDS engine. And, and I will also run a simple SQL statement. Okay. So let's it's time for the demo and let's move on to the demo. So moving on to the demo, let me show you what is already configured in order to save some time. So you can see here that I have already configured an Aurora serverless which is my SQL compatible. And this one, this end point is actually running in my default VPC using all three subnets. And this is my security group. So if I show you the security group configuration it should be somewhere here. Yeah. Here's my security group configuration. So if you look at over here it is actually taking my SQL traffic only from the default VPC. And then it is also doing self refreshing, self referencing, which is one of the requirement when you're trying to use a glue connection. Now here is in this RDS instance, I have created a database, dojo DB, and there I have created a table, my friends. And if I see a number records in my friends, you will find that, well, okay. So this one is okay. There's a connection error. So probably I need to connect again. Let me connect one more time. Okay. It looks like my connection is broken. So let me connect one more time. And okay. And change database. Okay. Just to reestablish a new connection. And let's simply say select star from my friends. And you run it. And hopefully, yeah, you can see it is empty. So this is fine. So this is my table I'm going to work with. Now I have created three endpoints since I'm going to use PyMySQL and also AWS triangular. So I have created three endpoints. So one end point is for glue. One end point is for secrets manager and one is for S3. And if you click on one of the endpoints, you will find that it is using all three subnets where same VPC where the RDS engine is, it's using the same security group. Yeah. So yeah, that's how the configuration look like. And same is for secrets manager and S3 as well. Now you have seen the security group. Now I created a couple of IAM roles. So one I created a dozer glue role. And this dozer glue role will be used by actually data connection. Glue connection, in fact. So that has got only one permission that is AWS glue service role. And then I have created another role for the Lambda and Lambda has got actually three permissions. One is glue service role so that it can talk to the glue connection. It has got a Lambda basic execution role so that it can create a cloud watch logs. And third one it has got Lambda VPC access is beautiful role so that it can associate itself to the VPC and can create ENIs and those kinds of things. Now I also configured a dojo, a glue connection. And you can see that this is a JDBC kind of connection. This is the end point. I'm talking to same VPC where my RDS instances, I choose one of the subnet and same security group. And I prodded login name and password. And this connection was stabilized and tested to say that it is working. So these are things I have already configured so that we can focus on the Lambda coding part. So let's start with the Lambda coding. And first thing I will do is that I will actually use AWS Wrangler. So this is my Lambda console. If I go to my layers, I have configured already a layer for AWS Wrangler. And again, if you want to know how it works, actually I have another tutorials which talks about using AWS Wranglers with Lambda. You might want to refer to that and that will tell you how to use AWS, how to configure AWS Wrangler with Lambda using Lambda layer. Okay. But here in this case, I have already got a version 3.8 compatible AWS Wrangler layer configured for the Lambda. So now I can start creating my function. So I create my first function and this function I will author from the scratch and I'll say this is my, you can say, okay, Wrangler function. And then this one is Python 3.8. And then I changed my execution role to the IAM role I created for the Lambda. In this case, it is Dojo Lambda role and I create the function. And the function should be created in no time. The function has now been created. Now, first thing I need to do is that I need to, since in this method, I'm going to use AWS Wrangler. So first thing I need to, I need to add the layer, which I have configured for the Wrangler, AWS Wrangler. So I click on the layers link and I say, I want to add a layer. And I say, let's use a custom layer. This is my AWS Wrangler layer. I've got only one version. Let's add it. So now my AWS Lambda function has got a layer configured over here. Now the next step is to, once my layer is configured, since I'm talking to database, I think I would like to go and increase the timeout period to a little, little larger. So first I go to my journal configuration over here and say, okay, let's increase that to, okay, just to be safe aside one minute, but it doesn't need one minute. Probably five 10 seconds is enough, that two for the first execution. But I increase the timeout and then I want to attach this Lambda function to the VPC because it has to talk to the RDS. So I need to attach this Lambda function to the VPC. So I go to the VPC section over here. I want to say edit and I want to use a default VPC because that's where my RDS database is. And I can select, yeah, all three subnets. Yeah. If you want, and I can select here, the MySQL security group, the same security group, other things are using, I simply unified everything to same security group. So I simply attach that security group and I save it. Now what's going to do that, it's going to associate my Lambda function with the VPC. So what happens behind the scene is that actually one ENI will be created for this Lambda function in the same VPC where, in the same VPC where your RDS database is running. And through that ENI, now my Lambda function can now make call to the RDS actually. So let's wait till this Lambda function gets associated with the VPC of the RDS database instance. And then we will simply upload, change the code and then we will test how it works. So yeah, let's wait till this configuration. So I have paused my video in the meantime, but now my Lambda function is associated with the VPC. Now I can change, I'm all good. I can change my code. You can see that it has got a VPC configuration done. So I can go to the code section here and say, and simply say, I want to change the code and I replace this code. Okay. Let me go a little up. I replace this code with, yeah, another code. So here we go. So I will provide the snippet of this code in the description box below, but it's pretty straightforward code guys. So I'm simply importing the AWS Wrangler function. Then I'm using Dojo connections. In this case, I have not put Dojo connection name or like my Glue connection name in the involvement variable, just to save some time. I'm simply hard coding it over here, but I'm using a Data Wrangler library to connect to the MySQL database using the Glue connection. Then I'm creating a cursor and is getting this small query, which is going to insert a recording to my table, commit and close. That's it. Pretty straightforward. So let's deploy this code and then we are going to test it. So deployment is done. Now we are going to test it. So when you click on test, it's asking me to configure a test. So there is no input. So I can say, okay, this is simple test. And yeah, let's create this test. And then you simply run the test. And when I run the test, hopefully everything is fine. And you can see that it has responded back there with data inserted. So if I go to my RDS instance and run this query again, I should see a code I should see a row inserted over here, first name one and last name one. Okay. So again, I will provide this code snippet so you can play with yourself in the description box below. Now, having tested one Lambda function, let's go and create the other Lambda function, which is going to use actually PyMySQL library. So for that, first we will create a package for PyMySQL. So here is my folder where I have got my Lambda function already actually created. And I'll, when I upload, I'll show you the code, but I have to configure my package and zip it. And then I can, then I can create my whole Lambda function. So I'm not using the layer method for the second example, because I want to save some time to be very honest. But let's run, let's create a local deployment package. And for that, we will simply run a command and the command is pip install hyphen t dot PyMySQL. So basically this command. So it's saying that, Hey, I want to install PyMySQL, but my target is a local directory simple. Yeah. And I am in c AWS dojo folder and I'm simply running this command and it will actually create a local deployment distribution for PyMySQL. So you can see here, these folders got created. Now I need to simply select these two folders and my Lambda function code and simply send it to zip. Okay. So I simply send it to zip and I'm ready. Now let's go back to my Lambda function and say, I want to create a new Lambda function. And this one I will say SQL Lambda. Yeah. SQL Lambda. Yeah. And if I come down here, I can select AWS Python 3.8 permission wise, again, nothing changes. I gave the same permission in this case and I give it dojo, oops, where is that? I do that dojo Lambda function, Lambda role and I create the role. So once I have created the role, my role is, is, is created. Now let's do well, you can see here that yeah, right now this one is using the default one, but this is not what I want to use. So what I will do is that I will simply go over here and say, I want to upload a package from my local directory. And in this case, I come over here and okay. I'm it's, I think it's going to a different folder. So I will simply ask it to go to another folder. And then this was my zip file. I simply save it. And what it is going to do, it's going to take my local, local package and it's going to upload it to the Lambda function. Again, guys, I'm not using the best practice here. I ideally I should have put zip these two folders, put into a layer and then add that layer over here. But that would have taken some time. So I tried to avoid that for the demo purposes. So let's see the code. If I wrap the line over here, you can see the lines of course are different here. So in this case, I'm using PI MySQL, and then I'm using PI MySQL.connect method to actually connect. And I have to provide my host name, username, database name, password, and my cursor class. And again, another best practice I'm not using here is that I should have put my database name and host name into environment variable because they are non sensitive information and my username and password into secrets manager. But again, I try to save some time for the video. So I simply hardcoded those values over here, but you should not be doing that. Okay. I'm sorry. I'm showing this thing here, but yeah, you don't do that. You keep your information in the secrets manager. And then yeah, I'm simply, I'm creating a cursor, executing a SQL statement with the cursor. Unless for the sake of testing, make it a two and two, and then yeah, commit and close. And that's it. So I simply going to deploy this thing here. So now my code is deployed, but I'm not ready yet because I need to configure my VPC endpoint. VPC is not VPC endpoint. I need to associate VPC with the Lambda function. And I also need to increase time out a bit. So let's go to the configuration and let's first go to the general configuration. I want to increase the timeout. Let's again choose a simple path, make it one minute, save it. Then you come down to the VPC section over here. Say I want to edit it and select my default VPC, my all three subnets. Yeah. It has to be your best practice. And then you select, yeah, my SQL security group over here. So that configuration is same whether whichever packages you're using and you simply save it. Now, again, it is going to associate my VPC with my Lambda function with the VPC and it's going to take some while. So we'll wait till the Lambda function gets associated with the, okay. So my Lambda function is not associated with the VPC and it happened really fast this time. Okay. So now my configuration is ready. I have got my Lambda function associated with the VPC and my code is also uploaded, which is actually my PyMySQL distribution has been packaged along with my code. And my code is pretty straightforward, which is connecting to the instance. And then I'm simply executing a simple insert statement. So let's run this code and hopefully it works. Again, it will ask to configure the test event. And I simply create it and then I test it and hopefully it works. Yeah, it worked. And if I go back to my RDS database and I run my query again, you can see that it would have got another row inserted and that is first name to and last name to, which showed that my Lambda function has worked. So this was an example, guys, how you can configure your Lambda function to work with RDS. And you learned two methods. One is using packages like PyMySQL to talk to the RDS engine. And I also learned how to use AWS data Wrangler to talk to the RDS databases. And I told you that, I mean, I am not recommending use one over another, but if you ask my personal choice, I prefer AWS Wrangler over the other packages because it's pretty simple to use and number of lines of code I write is less. And I can also use Panda data frame to manipulate my data, which actually makes life easy, especially if you're trying to perform any kind of data processing kind of operation. So that was all for today, guys. Hope you like this video. And if you like, please click on the like button. Please subscribe to my channel. And if you visit our website aws.com, you'll find many other videos and tutorials similar to like this. We always look forward to your feedback, whether it is for a new content or you want to discuss something we already talked about in a tutorial or we have made some mistake and you want to correct us, please do so. So we always look forward to your feedback to improve our quality to improve our delivery to you. So please provide your feedback in the description box of the YouTube channel. And I promise to come back again in a week's time with another such tutorial. In the meantime, have a nice day and stay safe. Bye bye.",
  "detected_language": "english"
}
